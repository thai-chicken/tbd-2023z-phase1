{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a0ed7a-a23a-4523-9171-f1d076700168",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_BUCKET=tbd-2023z-304098-data\n",
      "env: GEN_OUTPUT_DIR=/tmp/tpc-di\n",
      "env: REPO_ROOT=/home/jupyter/git/tbd-tpc-di/\n"
     ]
    }
   ],
   "source": [
    "%env DATA_BUCKET=tbd-2023z-304098-data\n",
    "%env GEN_OUTPUT_DIR=/tmp/tpc-di\n",
    "%env REPO_ROOT=/home/jupyter/git/tbd-tpc-di/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbec0d-d410-4de9-8dfd-00b07fdb7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3.8 install typer[All]==0.9.0 google-cloud-storage==2.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b5820-852d-4c30-a178-9dd78bbdbd5f",
   "metadata": {},
   "source": [
    "## Install SDKMAN for setting up JVM 8 enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3cdb5b-4978-479e-85c7-c9dbac3e65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -s https://get.sdkman.io | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691084f8-a883-408a-8d16-cc717669c04d",
   "metadata": {},
   "source": [
    "## Install and set as default JVM 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275488e6-68e3-4f42-a3ca-060b286bade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 8.0.392-amzn\n",
    "sdk use java 8.0.392-amzn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554db58-b832-45e2-a1f6-28bd873ae31d",
   "metadata": {},
   "source": [
    "## Check if JVM 8 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898835b7-c347-49dd-bc0f-ca845375e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_392\"\n",
      "OpenJDK Runtime Environment Corretto-8.392.08.1 (build 1.8.0_392-b08)\n",
      "OpenJDK 64-Bit Server VM Corretto-8.392.08.1 (build 25.392-b08, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45d9a3-e599-4661-944b-1bdfe3808c19",
   "metadata": {},
   "source": [
    "## Clone tbd-tpc-di repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7481d445-4ea6-40cc-8320-5520fe8d4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'tbd-tpc-di' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Already on 'notebook'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\ttests/fact_watches_dates_proper_relation.sql\n",
      "Your branch is up to date with 'origin/notebook'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p git && cd git\n",
    "git clone https://github.com/thai-chicken/tbd-tpc-di.git\n",
    "cd tbd-tpc-di\n",
    "git pull\n",
    "git checkout notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b992bfc-8f8c-42eb-bf07-069b0d897fda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate input dataset (run this cell below from the terminal!!!)\n",
    "It should take approx. 15min with scale factor set to 100 and generate approx. 10GiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ab901-15dc-4e94-b4fd-e2b9cfc6e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd /home/jupyter/git/tbd-tpc-di/tools/ \n",
    "java -jar DIGen.jar -sf 10 -o /tmp/tpc-di"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f948b2f-3c22-46a3-988f-23e97fa00668",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install and setup JVM 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e466e8-41aa-4afc-8392-5675d36adcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;33mjava 11.0.21-amzn is already installed.\u001b[0m\n",
      "\n",
      "\u001b[1;32mUsing java version 11.0.21-amzn in this shell.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 11.0.21-amzn\n",
    "sdk use java 11.0.21-amzn -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccda4c-40d4-40fe-9d2e-68b416522f64",
   "metadata": {},
   "source": [
    "## Load staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259b72bc-2efd-405b-883f-618a9772bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8f25649b-c864-4cb5-8879-1409c34c6e00;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 694ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8f25649b-c864-4cb5-8879-1409c34c6e00\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/19ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/04 22:12:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/04 22:12:15 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/01/04 22:12:16 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/01/04 22:12:24 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:12:24 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:12:24 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/01/04 22:12:24 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:12:24 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:12:50 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "24/01/04 22:14:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "DATE table created.\n",
      "DAILY_MARKET table created.\n",
      "INDUSTRY table created.\n",
      "PROSPECT table created.\n",
      "CUSTOMER_MGMT table created.\n",
      "TAX_RATE table created.\n",
      "HR table created.\n",
      "WATCH_HISTORY table created.\n",
      "TRADE table created.\n",
      "TRADE_HISTORY table created.\n",
      "STATUS_TYPE table created.\n",
      "TRADE_TYPE table created.\n",
      "HOLDING_HISTORY table created.\n",
      "CASH_TRANSACTION table created.\n",
      "CMP table created.\n",
      "SEC table created.\n",
      "FIN table created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd $REPO_ROOT\n",
    "python3.8 tpcdi.py --output-directory $GEN_OUTPUT_DIR --stage $DATA_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0740193-5da4-4aac-9349-aa4e76f4e99b",
   "metadata": {},
   "source": [
    "## Run dbt ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d0642c-b5c9-4aa8-996f-40f82e3c90e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m22:18:19  Running with dbt=1.7.3\n",
      "\u001b[0m22:18:19  Installing dbt-labs/dbt_utils\n",
      "\u001b[0m22:18:20  Installed from version 1.1.1\n",
      "\u001b[0m22:18:20  Up to date!\n",
      "\u001b[0m22:18:24  Running with dbt=1.7.3\n",
      "\u001b[0m22:18:25  Registered adapter: spark=1.7.1\n",
      "\u001b[0m22:18:25  Found 44 models, 4 tests, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m22:18:25  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-22f21f88-292d-4b32-aed2-323dee8b367e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 571ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-22f21f88-292d-4b32-aed2-323dee8b367e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/12ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/04 22:18:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/04 22:18:34 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/01/04 22:18:34 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/01/04 22:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/01/04 22:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:19:09 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m22:19:14  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m22:19:14  \n",
      "\u001b[0m22:19:14  1 of 43 START sql table model demo_bronze.brokerage_cash_transaction ........... [RUN]\n",
      "24/01/04 22:19:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/01/04 22:19:16 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m22:19:31  1 of 43 OK created sql table model demo_bronze.brokerage_cash_transaction ...... [\u001b[32mOK\u001b[0m in 17.35s]\n",
      "\u001b[0m22:19:31  2 of 43 START sql table model demo_bronze.brokerage_daily_market ............... [RUN]\n",
      "24/01/04 22:19:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:19:45  2 of 43 OK created sql table model demo_bronze.brokerage_daily_market .......... [\u001b[32mOK\u001b[0m in 13.72s]\n",
      "\u001b[0m22:19:45  3 of 43 START sql table model demo_bronze.brokerage_holding_history ............ [RUN]\n",
      "24/01/04 22:19:46 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:19:48  3 of 43 OK created sql table model demo_bronze.brokerage_holding_history ....... [\u001b[32mOK\u001b[0m in 2.91s]\n",
      "\u001b[0m22:19:48  4 of 43 START sql table model demo_bronze.brokerage_trade ...................... [RUN]\n",
      "24/01/04 22:19:48 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:19:57  4 of 43 OK created sql table model demo_bronze.brokerage_trade ................. [\u001b[32mOK\u001b[0m in 8.81s]\n",
      "\u001b[0m22:19:57  5 of 43 START sql table model demo_bronze.brokerage_trade_history .............. [RUN]\n",
      "24/01/04 22:19:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:03  5 of 43 OK created sql table model demo_bronze.brokerage_trade_history ......... [\u001b[32mOK\u001b[0m in 6.26s]\n",
      "\u001b[0m22:20:03  6 of 43 START sql table model demo_bronze.brokerage_watch_history .............. [RUN]\n",
      "24/01/04 22:20:04 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:11  6 of 43 OK created sql table model demo_bronze.brokerage_watch_history ......... [\u001b[32mOK\u001b[0m in 8.13s]\n",
      "\u001b[0m22:20:11  7 of 43 START sql table model demo_bronze.crm_customer_mgmt .................... [RUN]\n",
      "24/01/04 22:20:12 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "24/01/04 22:20:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "\u001b[0m22:20:15  7 of 43 OK created sql table model demo_bronze.crm_customer_mgmt ............... [\u001b[32mOK\u001b[0m in 3.65s]\n",
      "\u001b[0m22:20:15  8 of 43 START sql table model demo_bronze.finwire_company ...................... [RUN]\n",
      "24/01/04 22:20:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:17  8 of 43 OK created sql table model demo_bronze.finwire_company ................. [\u001b[32mOK\u001b[0m in 1.85s]\n",
      "\u001b[0m22:20:17  9 of 43 START sql table model demo_bronze.finwire_financial .................... [RUN]\n",
      "24/01/04 22:20:17 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:28  9 of 43 OK created sql table model demo_bronze.finwire_financial ............... [\u001b[32mOK\u001b[0m in 11.16s]\n",
      "\u001b[0m22:20:28  10 of 43 START sql table model demo_bronze.finwire_security .................... [RUN]\n",
      "24/01/04 22:20:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:30  10 of 43 OK created sql table model demo_bronze.finwire_security ............... [\u001b[32mOK\u001b[0m in 1.97s]\n",
      "\u001b[0m22:20:30  11 of 43 START sql table model demo_bronze.hr_employee ......................... [RUN]\n",
      "24/01/04 22:20:30 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:31  11 of 43 OK created sql table model demo_bronze.hr_employee .................... [\u001b[32mOK\u001b[0m in 1.47s]\n",
      "\u001b[0m22:20:31  12 of 43 START sql table model demo_bronze.reference_date ...................... [RUN]\n",
      "24/01/04 22:20:32 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:33  12 of 43 OK created sql table model demo_bronze.reference_date ................. [\u001b[32mOK\u001b[0m in 1.44s]\n",
      "\u001b[0m22:20:33  13 of 43 START sql table model demo_bronze.reference_industry .................. [RUN]\n",
      "24/01/04 22:20:33 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:34  13 of 43 OK created sql table model demo_bronze.reference_industry ............. [\u001b[32mOK\u001b[0m in 1.08s]\n",
      "\u001b[0m22:20:34  14 of 43 START sql table model demo_bronze.reference_status_type ............... [RUN]\n",
      "24/01/04 22:20:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:35  14 of 43 OK created sql table model demo_bronze.reference_status_type .......... [\u001b[32mOK\u001b[0m in 1.09s]\n",
      "\u001b[0m22:20:35  15 of 43 START sql table model demo_bronze.reference_tax_rate .................. [RUN]\n",
      "24/01/04 22:20:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:36  15 of 43 OK created sql table model demo_bronze.reference_tax_rate ............. [\u001b[32mOK\u001b[0m in 1.08s]\n",
      "\u001b[0m22:20:36  16 of 43 START sql table model demo_bronze.reference_trade_type ................ [RUN]\n",
      "24/01/04 22:20:37 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:37  16 of 43 OK created sql table model demo_bronze.reference_trade_type ........... [\u001b[32mOK\u001b[0m in 1.00s]\n",
      "\u001b[0m22:20:37  17 of 43 START sql table model demo_bronze.syndicated_prospect ................. [RUN]\n",
      "24/01/04 22:20:38 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:20:39  17 of 43 OK created sql table model demo_bronze.syndicated_prospect ............ [\u001b[32mOK\u001b[0m in 1.55s]\n",
      "\u001b[0m22:20:39  18 of 43 START sql table model demo_silver.daily_market ........................ [RUN]\n",
      "24/01/04 22:20:39 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:22:56  18 of 43 OK created sql table model demo_silver.daily_market ................... [\u001b[32mOK\u001b[0m in 137.26s]\n",
      "\u001b[0m22:22:56  19 of 43 START sql table model demo_silver.employees ........................... [RUN]\n",
      "24/01/04 22:22:56 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:22:58  19 of 43 OK created sql table model demo_silver.employees ...................... [\u001b[32mOK\u001b[0m in 1.81s]\n",
      "\u001b[0m22:22:58  20 of 43 START sql table model demo_silver.date ................................ [RUN]\n",
      "24/01/04 22:22:58 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:23:00  20 of 43 OK created sql table model demo_silver.date ........................... [\u001b[32mOK\u001b[0m in 1.86s]\n",
      "\u001b[0m22:23:00  21 of 43 START sql table model demo_silver.companies ........................... [RUN]\n",
      "24/01/04 22:23:00 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:23:05  21 of 43 OK created sql table model demo_silver.companies ...................... [\u001b[32mOK\u001b[0m in 5.69s]\n",
      "\u001b[0m22:23:06  22 of 43 START sql table model demo_silver.accounts ............................ [RUN]\n",
      "24/01/04 22:23:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:23:12  22 of 43 OK created sql table model demo_silver.accounts ....................... [\u001b[32mOK\u001b[0m in 6.25s]\n",
      "\u001b[0m22:23:12  23 of 43 START sql table model demo_silver.customers ........................... [RUN]\n",
      "24/01/04 22:23:12 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:23:19  23 of 43 OK created sql table model demo_silver.customers ...................... [\u001b[32mOK\u001b[0m in 6.45s]\n",
      "\u001b[0m22:23:19  24 of 43 START sql table model demo_silver.trades_history ...................... [RUN]\n",
      "24/01/04 22:23:19 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:24:11  24 of 43 OK created sql table model demo_silver.trades_history ................. [\u001b[32mOK\u001b[0m in 52.29s]\n",
      "\u001b[0m22:24:11  25 of 43 START sql table model demo_gold.dim_broker ............................ [RUN]\n",
      "24/01/04 22:24:12 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:24:14  25 of 43 OK created sql table model demo_gold.dim_broker ....................... [\u001b[32mOK\u001b[0m in 2.34s]\n",
      "\u001b[0m22:24:14  26 of 43 START sql table model demo_gold.dim_date .............................. [RUN]\n",
      "24/01/04 22:24:14 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:24:15  26 of 43 OK created sql table model demo_gold.dim_date ......................... [\u001b[32mOK\u001b[0m in 1.66s]\n",
      "\u001b[0m22:24:15  27 of 43 START sql table model demo_gold.dim_company ........................... [RUN]\n",
      "24/01/04 22:24:15 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:24:17  27 of 43 OK created sql table model demo_gold.dim_company ...................... [\u001b[32mOK\u001b[0m in 1.90s]\n",
      "\u001b[0m22:24:17  28 of 43 START sql table model demo_silver.financials .......................... [RUN]\n",
      "24/01/04 22:24:17 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:24:29  28 of 43 OK created sql table model demo_silver.financials ..................... [\u001b[32mOK\u001b[0m in 11.47s]\n",
      "\u001b[0m22:24:29  29 of 43 START sql table model demo_silver.securities .......................... [RUN]\n",
      "24/01/04 22:24:29 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:24:32  29 of 43 OK created sql table model demo_silver.securities ..................... [\u001b[32mOK\u001b[0m in 3.60s]\n",
      "\u001b[0m22:24:32  30 of 43 START sql table model demo_silver.cash_transactions ................... [RUN]\n",
      "24/01/04 22:24:33 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:24:39  30 of 43 OK created sql table model demo_silver.cash_transactions .............. [\u001b[32mOK\u001b[0m in 7.06s]\n",
      "\u001b[0m22:24:39  31 of 43 START sql table model demo_gold.dim_customer .......................... [RUN]\n",
      "24/01/04 22:24:40 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:24:45  31 of 43 OK created sql table model demo_gold.dim_customer ..................... [\u001b[32mOK\u001b[0m in 5.64s]\n",
      "\u001b[0m22:24:45  32 of 43 START sql table model demo_gold.dim_trade ............................. [RUN]\n",
      "24/01/04 22:24:45 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:25:05  32 of 43 OK created sql table model demo_gold.dim_trade ........................ [\u001b[32mOK\u001b[0m in 20.51s]\n",
      "\u001b[0m22:25:05  33 of 43 START sql table model demo_silver.trades .............................. [RUN]\n",
      "24/01/04 22:25:06 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:25:40  33 of 43 OK created sql table model demo_silver.trades ......................... [\u001b[32mOK\u001b[0m in 34.06s]\n",
      "\u001b[0m22:25:40  34 of 43 START sql table model demo_gold.dim_security .......................... [RUN]\n",
      "24/01/04 22:25:40 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:25:41  34 of 43 OK created sql table model demo_gold.dim_security ..................... [\u001b[32mOK\u001b[0m in 1.93s]\n",
      "\u001b[0m22:25:41  35 of 43 START sql table model demo_silver.watches_history ..................... [RUN]\n",
      "24/01/04 22:25:42 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:25:54  35 of 43 OK created sql table model demo_silver.watches_history ................ [\u001b[32mOK\u001b[0m in 12.73s]\n",
      "\u001b[0m22:25:54  36 of 43 START sql table model demo_gold.dim_account ........................... [RUN]\n",
      "24/01/04 22:25:55 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:25:57  36 of 43 OK created sql table model demo_gold.dim_account ...................... [\u001b[32mOK\u001b[0m in 3.07s]\n",
      "\u001b[0m22:25:57  37 of 43 START sql table model demo_silver.holdings_history .................... [RUN]\n",
      "24/01/04 22:25:58 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:26:10  37 of 43 OK created sql table model demo_silver.holdings_history ............... [\u001b[32mOK\u001b[0m in 12.70s]\n",
      "\u001b[0m22:26:10  38 of 43 START sql table model demo_silver.watches ............................. [RUN]\n",
      "24/01/04 22:26:10 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:26:29  38 of 43 OK created sql table model demo_silver.watches ........................ [\u001b[32mOK\u001b[0m in 19.37s]\n",
      "\u001b[0m22:26:29  39 of 43 START sql table model demo_gold.fact_cash_transactions ................ [RUN]\n",
      "24/01/04 22:26:30 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:26:36  39 of 43 OK created sql table model demo_gold.fact_cash_transactions ........... [\u001b[32mOK\u001b[0m in 6.11s]\n",
      "\u001b[0m22:26:36  40 of 43 START sql table model demo_gold.fact_trade ............................ [RUN]\n",
      "24/01/04 22:26:36 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:26:53  40 of 43 OK created sql table model demo_gold.fact_trade ....................... [\u001b[32mOK\u001b[0m in 17.67s]\n",
      "\u001b[0m22:26:53  41 of 43 START sql table model demo_gold.fact_holdings ......................... [RUN]\n",
      "24/01/04 22:26:54 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:27:23  41 of 43 OK created sql table model demo_gold.fact_holdings .................... [\u001b[32mOK\u001b[0m in 29.41s]\n",
      "\u001b[0m22:27:23  42 of 43 START sql table model demo_gold.fact_watches .......................... [RUN]\n",
      "24/01/04 22:27:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:27:30  42 of 43 OK created sql table model demo_gold.fact_watches ..................... [\u001b[32mOK\u001b[0m in 7.63s]\n",
      "\u001b[0m22:27:30  43 of 43 START sql table model demo_gold.fact_cash_balances .................... [RUN]\n",
      "24/01/04 22:27:31 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m22:27:41  43 of 43 OK created sql table model demo_gold.fact_cash_balances ............... [\u001b[32mOK\u001b[0m in 10.93s]\n",
      "\u001b[0m22:27:41  \n",
      "\u001b[0m22:27:41  Finished running 43 table models in 0 hours 9 minutes and 15.90 seconds (555.90s).\n",
      "\u001b[0m22:27:41  \n",
      "\u001b[0m22:27:41  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m22:27:41  \n",
      "\u001b[0m22:27:41  Done. PASS=43 WARN=0 ERROR=0 SKIP=0 TOTAL=43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt deps\n",
    "dbt run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae93304-68eb-46c8-86a3-26feec06a54a",
   "metadata": {},
   "source": [
    "## Run dbt tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b563f4-0b31-40a3-b353-9e6c9de8c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m22:27:47  Running with dbt=1.7.3\n",
      "\u001b[0m22:27:48  Installing dbt-labs/dbt_utils\n",
      "\u001b[0m22:27:48  Installed from version 1.1.1\n",
      "\u001b[0m22:27:48  Up to date!\n",
      "\u001b[0m22:27:52  Running with dbt=1.7.3\n",
      "\u001b[0m22:27:53  Registered adapter: spark=1.7.1\n",
      "\u001b[0m22:27:53  Found 44 models, 4 tests, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m22:27:53  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3fcec813-3c46-422b-815b-97d9de5f26b1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 555ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3fcec813-3c46-422b-815b-97d9de5f26b1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/31ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/04 22:27:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/04 22:28:04 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/01/04 22:28:04 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/01/04 22:28:14 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:28:14 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:28:14 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/01/04 22:28:14 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:28:14 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "24/01/04 22:28:37 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m22:28:41  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m22:28:41  \n",
      "\u001b[0m22:28:41  1 of 4 START test fact_trade__unique_trade ..................................... [RUN]\n",
      "24/01/04 22:28:43 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m22:28:56  1 of 4 PASS fact_trade__unique_trade ........................................... [\u001b[32mPASS\u001b[0m in 15.38s]\n",
      "\u001b[0m22:28:56  2 of 4 START test fact_trade__unique_trade-checkpoint .......................... [RUN]\n",
      "\u001b[0m22:29:00  2 of 4 PASS fact_trade__unique_trade-checkpoint ................................ [\u001b[32mPASS\u001b[0m in 3.69s]\n",
      "\u001b[0m22:29:00  3 of 4 START test fact_watches__dates_proper_relation .......................... [RUN]\n",
      "\u001b[0m22:29:03  3 of 4 PASS fact_watches__dates_proper_relation ................................ [\u001b[32mPASS\u001b[0m in 3.48s]\n",
      "\u001b[0m22:29:03  4 of 4 START test fact_watches__dates_proper_relation-checkpoint ............... [RUN]\n",
      "\u001b[0m22:29:05  4 of 4 PASS fact_watches__dates_proper_relation-checkpoint ..................... [\u001b[32mPASS\u001b[0m in 1.95s]\n",
      "\u001b[0m22:29:05  \n",
      "\u001b[0m22:29:05  Finished running 4 tests in 0 hours 1 minutes and 11.83 seconds (71.83s).\n",
      "\u001b[0m22:29:05  \n",
      "\u001b[0m22:29:05  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m22:29:05  \n",
      "\u001b[0m22:29:05  Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt deps\n",
    "dbt test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884478f-1b69-49e3-8b47-88582bb7e316",
   "metadata": {},
   "source": [
    "# Test Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ce00fe-ce46-4202-a798-a7f227ff3c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ccb06429-1b11-4007-8aea-a563e4eb9ebb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 607ms :: artifacts dl 34ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ccb06429-1b11-4007-8aea-a563e4eb9ebb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/27ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/05 00:15:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/05 00:15:42 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "24/01/05 00:15:42 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/01/05 00:15:51 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "24/01/05 00:15:51 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "24/01/05 00:15:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "24/01/05 00:15:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "24/01/05 00:15:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TBD-TPC-DI-setup\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c892605-9496-4526-b574-a19168678934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/05 00:16:20 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|     bronze|\n",
      "|    default|\n",
      "|demo_bronze|\n",
      "|  demo_gold|\n",
      "|demo_silver|\n",
      "|      digen|\n",
      "|       gold|\n",
      "|     silver|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e94c83-4983-49f5-8597-77e3b9c4661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use demo_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44296edd-82e6-4600-b730-d2cc4992a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|demo_gold|         dim_account|      false|\n",
      "|demo_gold|          dim_broker|      false|\n",
      "|demo_gold|         dim_company|      false|\n",
      "|demo_gold|        dim_customer|      false|\n",
      "|demo_gold|            dim_date|      false|\n",
      "|demo_gold|        dim_security|      false|\n",
      "|demo_gold|           dim_trade|      false|\n",
      "|demo_gold|  fact_cash_balances|      false|\n",
      "|demo_gold|fact_cash_transac...|      false|\n",
      "|demo_gold|       fact_holdings|      false|\n",
      "|demo_gold|          fact_trade|      false|\n",
      "|demo_gold|        fact_watches|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e2212-77da-431e-b2d4-a0fe50647240",
   "metadata": {},
   "source": [
    "# Calculate layers' tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27d30e8a-9a17-46d0-bc6a-1a5ea2970e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Layer |   # Tables\n",
      "----------------+-----------\n",
      "         bronze |          0\n",
      "        default |          0\n",
      "    demo_bronze |         17\n",
      "      demo_gold |         12\n",
      "    demo_silver |         14\n",
      "          digen |         17\n",
      "           gold |          0\n",
      "         silver |          0\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Layer':>15} | {'# Tables':>10}\")\n",
    "print(\"-\"*16 + \"+\" + \"-\"*11)\n",
    "for db_row in spark.sql(\"show databases\").collect():\n",
    "    db_name = db_row.namespace\n",
    "    spark.sql(f\"use {db_name}\")\n",
    "    n_tables = spark.sql(\"show tables\").count()\n",
    "    print(f\"{db_name:>15} | {n_tables:>10}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d431d55c-20d8-45ed-aa23-72824950a314",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf7793-ec51-45e9-8ea4-fe65dcd27dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = spark.sql(\"SELECT * FROM demo_gold.fact_watches LIMIT 10\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c7e1d-b9fe-4dab-8a12-29d3e8bf1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76205ed4-de8b-4aaa-9862-bd1f5e51f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''SELECT sk_trade_id, COUNT(*) cnt\n",
    "            FROM demo_gold.fact_trade\n",
    "            GROUP BY sk_trade_id\n",
    "            HAVING cnt > 1\n",
    "    ''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
